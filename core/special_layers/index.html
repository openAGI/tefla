<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Mrinal(Ishant) Haloi">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>Special Layers - Tefla</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  <link href="../../css/extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "Special Layers";
    var mkdocs_page_input_path = "core/special_layers.md";
    var mkdocs_page_url = "/core/special_layers/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Tefla</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../..">Home</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../..">Index</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../installation/">Installation</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Layers</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../layers/">Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../rnn_cell/">RNN</a>
        
    </li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="./">Special Layers</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#spatial-transformer-layer">Spatial Transformer Layer</a></li>
                
            
                <li class="toctree-l3"><a href="#subsamples-the-input-along-the-spatial-dimensions">Subsamples the input along the spatial dimensions</a></li>
                
            
                <li class="toctree-l3"><a href="#strided-2-d-convolution-with-same-padding">Strided 2-D convolution with 'SAME' padding</a></li>
                
            
                <li class="toctree-l3"><a href="#bottleneck-residual-unit-variant-with-bn-before-convolutions">Bottleneck residual unit variant with BN before convolutions</a></li>
                
            
                <li class="toctree-l3"><a href="#bottleneck-residual-unit-variant-with-bn-before-convolutions_1">Bottleneck residual unit variant with BN before convolutions</a></li>
                
            
                <li class="toctree-l3"><a href="#densecrf-over-unnormalised-predictions">DenseCRF over unnormalised predictions</a></li>
                
            
                <li class="toctree-l3"><a href="#resnext-block">ResNeXt Block</a></li>
                
            
                <li class="toctree-l3"><a href="#embedding">Embedding</a></li>
                
            
                <li class="toctree-l3"><a href="#gated-unit-for-language-modelling">Gated unit for language modelling</a></li>
                
            
                <li class="toctree-l3"><a href="#returns-glimpses-at-the-locations">Returns glimpses at the locations</a></li>
                
            
                <li class="toctree-l3"><a href="#adds-a-pva-block-layer">Adds a PVA block layer</a></li>
                
            
                <li class="toctree-l3"><a href="#adds-a-pva-block-v2-layer">Adds a PVA block v2 layer</a></li>
                
            
            </ul>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../layer_arg_ops/">Layer Args</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Learner</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../training/">Learner</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../learning/">Learner Multi GPU</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../learning_v2/">Learner Multi GPU V2</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../learning_ss/">Learner Semi Supervised</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Predictor</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../prediction/">Predictor</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Built-in Ops</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../lr_policy/">Lr_Policy</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../metrics/">Metrics</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../initializers/">Initializations</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../losses/">Losses</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../summary/">Summaries</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../logger/">Logger</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../iter_ops/">Iter Ops</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Data Management</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../da/data/">Data Augmentation</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../da/standardizer/">Standardizer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../dataset/image_to_tfrecords/">TfRecords</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../dataset/base/">Dataset</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../dataset/dataflow/">Dataflow</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../dataset/decoder/">Decoder</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../dataset/reader/">Reader</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Utils</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../utils/util/">Utils</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../contributions/">Contributions</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../license/">License</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Tefla</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Layers &raquo;</li>
        
      
    
    <li>Special Layers</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/n3011/tefla/edit/master/docs/core/special_layers.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="spatial-transformer-layer">Spatial Transformer Layer</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/special_layers.py#L10 target="_blank"><b>tefla.core.special_layers.spatialtransformer</b></a></span>  (U,  theta,  batch_size=64,  downsample_factor=1.0,  num_transform=1,  name='SpatialTransformer',  **kwargs)</span></p>
<p>Implements a spatial transformer layer as described in [1]<em>.
It's based on lasagne implementation in [2]</em>, modified by Mrinal Haloi</p>
<h3>Args</h3>

<ul>
<li><strong>U</strong>: float
The output of a convolutional net should have the
shape [batch_size, height, width, num_channels].</li>
<li><strong>theta</strong>: float
The output of the localisation network should be [batch_size, num_transform, 6] or [batch_size, 6] if num_transform=1
<code>python`theta`` to : - identity = np.array([[1., 0., 0.], -  [0., 1., 0.]]) - identity = identity.flatten() - theta = tf.Variable(initial_value=identity)</code></li>
<li><strong>downsample_factor</strong>: a float, determines output shape, downsample input shape by downsample_factor</li>
</ul>
<h3>Returns</h3>

<p>spatial transformed output of the network</p>
<hr />
<h1 id="subsamples-the-input-along-the-spatial-dimensions">Subsamples the input along the spatial dimensions</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/special_layers.py#L164 target="_blank"><b>tefla.core.special_layers.subsample</b></a></span>  (inputs,  factor,  name=None)</span></p>
<h3>Args</h3>

<ul>
<li><strong>inputs</strong>: A <code>Tensor</code> of size [batch, height_in, width_in, channels].</li>
<li><strong>factor</strong>: The subsampling factor.</li>
<li><strong>name</strong>: Optional variable_scope.</li>
</ul>
<h3>Returns</h3>

<p>output: A <code>Tensor</code> of size [batch, height_out, width_out, channels] with the
input, either intact (if factor == 1) or subsampled (if factor &gt; 1).</p>
<hr />
<h1 id="strided-2-d-convolution-with-same-padding">Strided 2-D convolution with 'SAME' padding</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/special_layers.py#L182 target="_blank"><b>tefla.core.special_layers.conv2d_same</b></a></span>  (inputs,  num_outputs,  kernel_size,  stride,  rate=1,  name=None,  **kwargs)</span></p>
<p>When stride &gt; 1, then we do explicit zero-padding, followed by conv2d with
'VALID' padding.</p>
<p>Note that</p>
<p>net = conv2d_same(inputs, num_outputs, 3, stride=stride)</p>
<p>is equivalent to</p>
<p>net = conv2d(inputs, num_outputs, 3, stride=1, padding='SAME')
   net = subsample(net, factor=stride)</p>
<p>whereas</p>
<p>net = conv2d(inputs, num_outputs, 3, stride=stride, padding='SAME')</p>
<p>is different when the input's height or width is even, which is why we add the
current function. For more details, see ResnetUtilsTest.testConv2DSameEven().</p>
<h3>Args</h3>

<ul>
<li><strong>inputs</strong>: A 4-D tensor of size [batch, height_in, width_in, channels].</li>
<li><strong>num_outputs</strong>: An integer, the number of output filters.</li>
<li><strong>kernel_size</strong>: An int with the kernel_size of the filters.</li>
<li><strong>stride</strong>: An integer, the output stride.</li>
<li><strong>rate</strong>: An integer, rate for atrous convolution.</li>
<li><strong>name</strong>: name.</li>
</ul>
<h3>Returns</h3>

<p>output: A 4-D tensor of size [batch, height_out, width_out, channels] with
the convolution output.</p>
<hr />
<h1 id="bottleneck-residual-unit-variant-with-bn-before-convolutions">Bottleneck residual unit variant with BN before convolutions</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/special_layers.py#L230 target="_blank"><b>tefla.core.special_layers.bottleneck_v1</b></a></span>  (inputs,  depth,  depth_bottleneck,  stride,  rate=1,  name=None,  **kwargs)</span></p>
<p>This is the full preactivation residual unit variant proposed in [2]. See
Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck
variant which has an extra bottleneck layer.</p>
<p>When putting together two consecutive ResNet blocks that use this unit, one
should use stride = 2 in the last unit of the first block.</p>
<h3>Args</h3>

<ul>
<li><strong>inputs</strong>: A tensor of size [batch, height, width, channels].</li>
<li><strong>depth</strong>: The depth of the ResNet unit output.</li>
<li><strong>depth_bottleneck</strong>: The depth of the bottleneck layers.</li>
<li><strong>stride</strong>: The ResNet unit's stride. Determines the amount of downsampling of
the units output compared to its input.</li>
<li><strong>rate</strong>: An integer, rate for atrous convolution.</li>
<li><strong>outputs_collections</strong>: Collection to add the ResNet unit output.</li>
<li><strong>name</strong>: Optional variable_scope.</li>
</ul>
<h3>Returns</h3>

<p>The ResNet unit's output.</p>
<hr />
<h1 id="bottleneck-residual-unit-variant-with-bn-before-convolutions_1">Bottleneck residual unit variant with BN before convolutions</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/special_layers.py#L277 target="_blank"><b>tefla.core.special_layers.bottleneck_v2</b></a></span>  (inputs,  depth,  depth_bottleneck,  stride,  rate=1,  name=None,  **kwargs)</span></p>
<p>This is the full preactivation residual unit variant proposed in [2]. See
Fig. 1(b) of [2] for its definition. Note that we use here the bottleneck
variant which has an extra bottleneck layer.</p>
<p>When putting together two consecutive ResNet blocks that use this unit, one
should use stride = 2 in the last unit of the first block.</p>
<h3>Args</h3>

<ul>
<li><strong>inputs</strong>: A tensor of size [batch, height, width, channels].</li>
<li><strong>depth</strong>: The depth of the ResNet unit output.</li>
<li><strong>depth_bottleneck</strong>: The depth of the bottleneck layers.</li>
<li><strong>stride</strong>: The ResNet unit's stride. Determines the amount of downsampling of
the units output compared to its input.</li>
<li><strong>rate</strong>: An integer, rate for atrous convolution.</li>
<li><strong>outputs_collections</strong>: Collection to add the ResNet unit output.</li>
<li><strong>name</strong>: Optional variable_scope.</li>
</ul>
<h3>Returns</h3>

<p>The ResNet unit's output.</p>
<hr />
<h1 id="densecrf-over-unnormalised-predictions">DenseCRF over unnormalised predictions</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/special_layers.py#L405 target="_blank"><b>tefla.core.special_layers.dense_crf</b></a></span>  (probs,  img=None,  n_classes=15,  n_iters=10,  sxy_gaussian=  (1,  1),  compat_gaussian=4,  kernel_gaussian=<KernelType.DIAG_KERNEL:  1>,  normalisation_gaussian=<NormalizationType.NORMALIZE_SYMMETRIC:  3>,  sxy_bilateral=  (49,  49),  compat_bilateral=2,  srgb_bilateral=  (13,  13,  13),  kernel_bilateral=<KernelType.DIAG_KERNEL:  1>,  normalisation_bilateral=<NormalizationType.NORMALIZE_SYMMETRIC:  3>)</span>
   More details on the arguments at https://github.com/lucasb-eyer/pydensecrf.</p>
<h3>Args</h3>

<ul>
<li><strong>probs</strong>: class probabilities per pixel.</li>
<li><strong>img</strong>: if given, the pairwise bilateral potential on raw RGB values will be computed.</li>
<li><strong>n_iters</strong>: number of iterations of MAP inference.</li>
<li><strong>sxy_gaussian</strong>: standard deviations for the location component
of the colour-independent term.</li>
<li><strong>compat_gaussian</strong>: label compatibilities for the colour-independent
term (can be a number, a 1D array, or a 2D array).</li>
<li><strong>kernel_gaussian</strong>: kernel precision matrix for the colour-independent
term (can take values CONST_KERNEL, DIAG_KERNEL, or FULL_KERNEL).</li>
<li><strong>normalisation_gaussian</strong>: normalisation for the colour-independent term
(possible values are NO_NORMALIZATION, NORMALIZE_BEFORE, NORMALIZE_AFTER, NORMALIZE_SYMMETRIC).</li>
<li><strong>sxy_bilateral</strong>: standard deviations for the location component of the colour-dependent term.</li>
<li><strong>compat_bilateral</strong>: label compatibilities for the colour-dependent
term (can be a number, a 1D array, or a 2D array).</li>
<li><strong>srgb_bilateral</strong>: standard deviations for the colour component
of the colour-dependent term.</li>
<li><strong>kernel_bilateral</strong>: kernel precision matrix for the colour-dependent term
(can take values CONST_KERNEL, DIAG_KERNEL, or FULL_KERNEL).</li>
<li><strong>normalisation_bilateral</strong>: normalisation for the colour-dependent term
(possible values are NO_NORMALIZATION, NORMALIZE_BEFORE, NORMALIZE_AFTER, NORMALIZE_SYMMETRIC).</li>
</ul>
<h3>Returns</h3>

<p>Refined predictions after MAP inference.</p>
<hr />
<h1 id="resnext-block">ResNeXt Block</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/special_layers.py#L487 target="_blank"><b>tefla.core.special_layers.resnext_block</b></a></span>  (inputs,  nb_blocks,  out_channels,  is_training,  reuse,  cardinality,  downsample=False,  downsample_strides=2,  activation=<function  relu  at  0x7f46a41127d0>,  batch_norm=None,  batch_norm_args=None,  name='ResNeXtBlock',  **kwargs)</span>
resnext paper https://arxiv.org/pdf/1611.05431.pdf</p>
<h3>Args</h3>

<ul>
<li><strong>inputs</strong>: <code>Tensor</code>. Inputs 4-D Layer.</li>
<li><strong>nb_blocks</strong>: <code>int</code>. Number of layer blocks.</li>
<li><strong>out_channels</strong>: <code>int</code>. The number of convolutional filters of the
layers surrounding the bottleneck layer.</li>
<li><strong>cardinality</strong>: <code>int</code>. Number of aggregated residual transformations.</li>
<li><strong>downsample</strong>: <code>bool</code>. If True, apply downsampling using
'downsample_strides' for strides.</li>
<li><strong>downsample_strides</strong>: <code>int</code>. The strides to use when downsampling.</li>
<li><strong>activation</strong>: <code>function</code> (returning a <code>Tensor</code>).</li>
<li><strong>batch_norm</strong>: <code>bool</code>. If True, apply batch normalization.</li>
<li>use_ bias: <code>bool</code>. If True, a bias is used.</li>
<li><strong>w_init</strong>: <code>function</code>, Weights initialization.</li>
<li><strong>b_init</strong>: <code>tf.Tensor</code>. Bias initialization.</li>
<li><strong>w_regularizer</strong>: <code>function</code>. Add a regularizer to this</li>
<li><strong>weight_decay</strong>: <code>float</code>. Regularizer decay parameter. Default: 0.001.</li>
<li><strong>trainable</strong>: <code>bool</code>. If True, weights will be trainable.</li>
<li><strong>reuse</strong>: <code>bool</code>. If True and 'scope' is provided, this layer variables
will be reused (shared).
override name.</li>
<li><strong>name</strong>: A name for this layer (optional). Default: 'ResNeXtBlock'.</li>
</ul>
<h3>Returns</h3>

<p>4-D Tensor [batch, new height, new width, out_channels].</p>
<hr />
<h1 id="embedding">Embedding</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/special_layers.py#L562 target="_blank"><b>tefla.core.special_layers.embedding</b></a></span>  (inputs,  vocab_dim,  embedding_dim,  reuse,  validate_indices=False,  w_init=<tensorflow.python.ops.init_ops.RandomUniform  object  at  0x7f468520c390>,  trainable=True,  normalize=False,  vocab_freqs=None,  name='Embedding')</span>
Embedding layer for a sequence of integer ids or floats.</p>
<h3>Args</h3>

<ul>
<li><strong>inputs</strong>: a 2-D <code>Tensor</code> [samples, ids].</li>
<li><strong>vocab_dim</strong>: list of <code>int</code>. Vocabulary size (number of ids).</li>
<li><strong>embedding_dim</strong>: list of <code>int</code>. Embedding size.</li>
<li><strong>validate_indices</strong>: <code>bool</code>. Whether or not to validate gather indices.</li>
<li><strong>w_init</strong>:  Weights initialization.</li>
<li><strong>trainable</strong>: <code>bool</code>. If True, weights will be trainable.</li>
<li><strong>reuse</strong>: <code>bool</code>. If True and 'scope' is provided, this layer variables
will be reused (shared).</li>
<li><strong>name</strong>: A name for this layer (optional). Default: 'Embedding'.</li>
</ul>
<h3>Returns</h3>

<p>3-D Tensor [samples, embedded_ids, features].</p>
<hr />
<h1 id="gated-unit-for-language-modelling">Gated unit for language modelling</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/special_layers.py#L613 target="_blank"><b>tefla.core.special_layers.gated_layer</b></a></span>  (inputs,  layer,  num_units,  is_training,  reuse,  name='gated_layer',  **kwargs)</span></p>
<h3>Args</h3>

<ul>
<li><strong>inputs</strong>: a 3-D/4-D <code>Tensor</code>, input [samples, timesteps, input_dim]</li>
<li><strong>layer</strong>: a <code>layer</code>, layer to pass the inputs
e.g. <code>tefla.core.layers</code></li>
<li><strong>num_units</strong>: a <code>int</code>, number of units for each layer</li>
<li><strong>is_training</strong>: a <code>boolean</code>, Training if its true</li>
<li><strong>reuse</strong>: <code>bool</code>. If True and 'scope' is provided, this layer variables
will be reused (shared).</li>
<li><strong>name</strong>: A name for this layer (optional). Default: 'gated_layer'.</li>
</ul>
<h3>Returns</h3>

<p>a 3-D/4-D <code>Tensor</code>, output of the gated unit</p>
<hr />
<h1 id="returns-glimpses-at-the-locations">Returns glimpses at the locations</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/special_layers.py#L643 target="_blank"><b>tefla.core.special_layers.glimpseSensor</b></a></span>  (img,  normLoc,  minRadius=4,  depth=1,  sensorBandwidth=12)</span></p>
<h3>Args</h3>

<ul>
<li><strong>img</strong>: a 4-D <code>Tensor</code>, [batch_size, width, height, channels]</li>
<li><strong>normloc</strong>: a <code>float</code>, [0, 1] normalized location</li>
<li><strong>minRadius</strong>: a <code>int</code>, min radius for zooming</li>
<li><strong>depth</strong>: a <code>int</code>, number of zooms</li>
<li><strong>sensorbandwidth</strong>: a <code>int</code>, output glimpse size, width/height</li>
</ul>
<h3>Returns</h3>

<p>a 5-D <code>tensor</code> of glimpses</p>
<hr />
<h1 id="adds-a-pva-block-layer">Adds a PVA block layer</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/special_layers.py#L690 target="_blank"><b>tefla.core.special_layers.pva_block_v1</b></a></span>  (x,  num_units,  name='pva_block_v1',  **kwargs)</span>
convolution followed by crelu and scaling</p>
<h3>Args</h3>

<ul>
<li><strong>x</strong>: A 4-D <code>Tensor</code> of with at least rank 2 and value for the last dimension,
i.e. <code>[batch_size, in_height, in_width, depth]</code>,</li>
<li><strong>is_training</strong>: Bool, training or testing</li>
<li><strong>num_units</strong>: Integer or long, the number of output units in the layer.</li>
<li><strong>reuse</strong>: whether or not the layer and its variables should be reused. To be
able to reuse the layer scope must be given.</li>
<li><strong>filter_size</strong>: a int or list/tuple of 2 positive integers specifying the spatial
dimensions of of the filters.</li>
<li><strong>stride</strong>: a int or tuple/list of 2 positive integers specifying the stride at which to
compute output.</li>
<li><strong>padding</strong>: one of <code>"VALID"</code> or <code>"SAME"</code>.</li>
<li><strong>activation</strong>: activation function, set to None to skip it and maintain
a linear activation.</li>
<li><strong>batch_norm</strong>: normalization function to use. If
<code>batch_norm</code> is <code>True</code> then google original implementation is used and
if another function is provided then it is applied.
default set to None for no normalizer function</li>
<li><strong>batch_norm_args</strong>: normalization function parameters.</li>
<li><strong>w_init</strong>: An initializer for the weights.</li>
<li><strong>w_regularizer</strong>: Optional regularizer for the weights.</li>
<li><strong>untie_biases</strong>: spatial dimensions wise baises</li>
<li><strong>b_init</strong>: An initializer for the biases. If None skip biases.</li>
<li><strong>trainable</strong>: If <code>True</code> also add variables to the graph collection
<code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li>
<li><strong>name</strong>: Optional name or scope for variable_scope/name_scope.</li>
<li><strong>use_bias</strong>: Whether to add bias or not</li>
</ul>
<h3>Returns</h3>

<p>The 4-D <code>Tensor</code> variable representing the result of the series of operations.
e.g.: 4-D <code>Tensor</code> [batch, new_height, new_width, n_output].</p>
<hr />
<h1 id="adds-a-pva-block-v2-layer">Adds a PVA block v2 layer</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/special_layers.py#L744 target="_blank"><b>tefla.core.special_layers.pva_block_v2</b></a></span>  (x,  num_units,  name='pva_block_v2',  **kwargs)</span>
first batch normalization followed by crelu and scaling, convolution is applied after scalling</p>
<h3>Args</h3>

<ul>
<li><strong>x</strong>: A 4-D <code>Tensor</code> of with at least rank 2 and value for the last dimension,
i.e. <code>[batch_size, in_height, in_width, depth]</code>,</li>
<li><strong>is_training</strong>: Bool, training or testing</li>
<li><strong>num_units</strong>: Integer or long, the number of output units in the layer.</li>
<li><strong>reuse</strong>: whether or not the layer and its variables should be reused. To be
able to reuse the layer scope must be given.</li>
<li><strong>filter_size</strong>: a int or list/tuple of 2 positive integers specifying the spatial
dimensions of of the filters.</li>
<li><strong>stride</strong>: a int or tuple/list of 2 positive integers specifying the stride at which to
compute output.</li>
<li><strong>padding</strong>: one of <code>"VALID"</code> or <code>"SAME"</code>.</li>
<li><strong>activation</strong>: activation function, set to None to skip it and maintain
a linear activation.</li>
<li><strong>batch_norm</strong>: normalization function to use. If
<code>batch_norm</code> is <code>True</code> then google original implementation is used and
if another function is provided then it is applied.
default set to None for no normalizer function</li>
<li><strong>batch_norm_args</strong>: normalization function parameters.</li>
<li><strong>w_init</strong>: An initializer for the weights.</li>
<li><strong>w_regularizer</strong>: Optional regularizer for the weights.</li>
<li><strong>untie_biases</strong>: spatial dimensions wise baises</li>
<li><strong>b_init</strong>: An initializer for the biases. If None skip biases.</li>
<li><strong>trainable</strong>: If <code>True</code> also add variables to the graph collection
<code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li>
<li><strong>name</strong>: Optional name or scope for variable_scope/name_scope.</li>
<li><strong>use_bias</strong>: Whether to add bias or not</li>
</ul>
<h3>Returns</h3>

<p>The 4-D <code>Tensor</code> variable representing the result of the series of operations.
e.g.: 4-D <code>Tensor</code> [batch, new_height, new_width, n_output].</p>
<hr />
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../layer_arg_ops/" class="btn btn-neutral float-right" title="Layer Args">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../rnn_cell/" class="btn btn-neutral" title="RNN"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/n3011/tefla" class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../rnn_cell/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../layer_arg_ops/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../js/theme.js"></script>

</body>
</html>
