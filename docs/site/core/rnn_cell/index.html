<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <meta name="author" content="Mrinal(Ishant) Haloi">
  <link rel="shortcut icon" href="../../img/favicon.ico">
  <title>RNN - Tefla</title>
  <link href='https://fonts.googleapis.com/css?family=Lato:400,700|Roboto+Slab:400,700|Inconsolata:400,700' rel='stylesheet' type='text/css'>

  <link rel="stylesheet" href="../../css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../css/theme_extra.css" type="text/css" />
  <link rel="stylesheet" href="../../css/highlight.css">
  <link href="../../css/extra.css" rel="stylesheet">
  
  <script>
    // Current page data
    var mkdocs_page_name = "RNN";
    var mkdocs_page_input_path = "core/rnn_cell.md";
    var mkdocs_page_url = "/core/rnn_cell/";
  </script>
  
  <script src="../../js/jquery-2.1.1.min.js"></script>
  <script src="../../js/modernizr-2.8.3.min.js"></script>
  <script type="text/javascript" src="../../js/highlight.pack.js"></script> 
  
</head>

<body class="wy-body-for-nav" role="document">

  <div class="wy-grid-for-nav">

    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side stickynav">
      <div class="wy-side-nav-search">
        <a href="../.." class="icon icon-home"> Tefla</a>
        <div role="search">
  <form id ="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
  </form>
</div>
      </div>

      <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
	<ul class="current">
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../..">Home</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../..">Index</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../installation/">Installation</a>
        
    </li>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Layers</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../layers/">Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 current">
        <a class="current" href="./">RNN</a>
        
            <ul>
            
                <li class="toctree-l3"><a href="#the-most-basic-rnn-cell">The most basic RNN cell</a></li>
                
            
                <li class="toctree-l3"><a href="#lstm-unit">LSTM unit</a></li>
                
            
                <li class="toctree-l3"><a href="#basic-attention-cell">Basic attention cell</a></li>
                
            
                <li class="toctree-l3"><a href="#gated-recurrent-unit-cell-cf-httparxivorgabs14061078">Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078)</a></li>
                
            
                <li class="toctree-l3"><a href="#rnn-cell-composed-sequentially-of-multiple-simple-cells">RNN cell composed sequentially of multiple simple cells</a></li>
                
            
                <li class="toctree-l3"><a href="#operator-adding-dropout-to-inputs-and-outputs-of-the-given-cell">Operator adding dropout to inputs and outputs of the given cell</a></li>
                
            
                <li class="toctree-l3"><a href="#adds-a-fully-connected-layer">Adds a fully connected layer</a></li>
                
            
                <li class="toctree-l3"><a href="#adds-a-layer-normalization-layer-from-httpsarxivorgabs160706450">Adds a Layer Normalization layer from https://arxiv.org/abs/1607.06450</a></li>
                
            
            </ul>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../special_layers/">Special Layers</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../layer_arg_ops/">Layer Args</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Trainer</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../training/">Trainer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../learning/">Trainer Multi GPU</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../learning_ss/">Trainer Semi Supervised</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Predictor</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../prediction/">Predictor</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Built-in Ops</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../lr_policy/">Lr_Policy</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../metrics/">Metrics</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../initializers/">Initializations</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../losses/">Losses</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../summary/">Summaries</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../logger/">Logger</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../iter_ops/">Iter Ops</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Data Management</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../da/data/">Data Augmentation</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../da/standardizer/">Standardizer</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../dataset/image_to_tfrecords/">TfRecords</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../dataset/base/">Dataset</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../dataset/dataflow/">Dataflow</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../dataset/decoder/">Decoder</a>
        
    </li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../dataset/reader/">Reader</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <ul class="subnav">
    <li><span>Utils</span></li>

        
            
    <li class="toctree-l1 ">
        <a class="" href="../../utils/util/">Utils</a>
        
    </li>

        
    </ul>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../contributions/">Contributions</a>
        
    </li>
<li>
          
            <li>
    <li class="toctree-l1 ">
        <a class="" href="../../license/">License</a>
        
    </li>
<li>
          
        </ul>
      </div>
      &nbsp;
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" role="navigation" aria-label="top navigation">
        <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
        <a href="../..">Tefla</a>
      </nav>

      
      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="breadcrumbs navigation">
  <ul class="wy-breadcrumbs">
    <li><a href="../..">Docs</a> &raquo;</li>
    
      
        
          <li>Layers &raquo;</li>
        
      
    
    <li>RNN</li>
    <li class="wy-breadcrumbs-aside">
      
        <a href="https://github.com/n3011/tefla/edit/master/docs/core/rnn_cell.md"
          class="icon icon-github"> Edit on GitHub</a>
      
    </li>
  </ul>
  <hr/>
</div>
          <div role="main">
            <div class="section">
              
                <h1 id="the-most-basic-rnn-cell">The most basic RNN cell</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/rnn_cell.py#L15 target="_blank"><b>tefla.core.rnn_cell.BasicRNNCell</b></a></span>  (num_units,  reuse,  trainable=True,  input_size=None,  activation=<function  tanh  at  0x7f0c75162de8>,  layer_norm=None,  layer_norm_args=None,  outputs_collections=None)</span></p>
<h3>Args</h3>

<ul>
<li><strong>num_units</strong>: int, The number of units in the LSTM cell.</li>
<li><strong>reuse</strong>: whether or not the layer and its variables should be reused. To be
able to reuse the layer scope must be given.</li>
<li><strong>input_size</strong>: Deprecated and unused.</li>
<li><strong>activation</strong>: Activation function of the states.</li>
<li><strong>layer_norm</strong>: If <code>True</code>, layer normalization will be applied.</li>
<li><strong>layer_norm_args</strong>: optional dict, layer_norm arguments</li>
<li><strong>trainable</strong>: If <code>True</code> also add variables to the graph collection
<code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li>
<li><strong>outputs_collections</strong>: The collections to which the outputs are added.</li>
</ul>
<h2>Methods</h2>

<p><span class="hr_large"></span> </p>
<p><span class="extra_h2"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/#L145 target="_blank"><b>zero_state</b></a></span>  (batch_size,  dtype)</span></p>
<h5>Args</h5>

<p>batch_size: int, float, or unit Tensor representing the batch size.
  dtype: the data type to use for the state.</p>
<h5>Returns</h5>

<p>If <code>state_size</code> is an int or TensorShape, then the return value is a
  <code>N-D</code> tensor of shape <code>[batch_size x state_size]</code> filled with zeros.</p>
<hr />
<h1 id="lstm-unit">LSTM unit</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/rnn_cell.py#L60 target="_blank"><b>tefla.core.rnn_cell.LSTMCell</b></a></span>  (num_units,  reuse,  trainable=True,  forget_bias=1.0,  input_size=None,  activation=<function  tanh  at  0x7f0c75162de8>,  inner_activation=<function  sigmoid  at  0x7f0c75162d70>,  keep_prob=1.0,  dropout_seed=None,  cell_clip=None,  layer_norm=None,  layer_norm_args=None,  outputs_collections=None)</span></p>
<p>This class adds layer normalization and recurrent dropout to a
basic LSTM unit. Layer normalization implementation is based on:
https://arxiv.org/abs/1607.06450.
"Layer Normalization" Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton
and is applied before the internal nonlinearities.
Recurrent dropout is base on:
https://arxiv.org/abs/1603.05118
"Recurrent Dropout without Memory Loss"
Stanislau Semeniuta, Aliaksei Severyn, Erhardt Barth.</p>
<h3>Args</h3>

<ul>
<li><strong>num_units</strong>: int, The number of units in the LSTM cell.</li>
<li><strong>reuse</strong>: whether or not the layer and its variables should be reused. To be
able to reuse the layer scope must be given.</li>
<li><strong>forget_bias</strong>: float, The bias added to forget gates (see above).</li>
<li><strong>input_size</strong>: Deprecated and unused.</li>
<li><strong>activation</strong>: Activation function of the states.</li>
<li><strong>inner_activation</strong>: Activation function of the inner states.</li>
<li><strong>layer_norm</strong>: If <code>True</code>, layer normalization will be applied.</li>
<li><strong>layer_norm_args</strong>: optional dict, layer_norm arguments</li>
<li><strong>cell_clip</strong>: (optional) A float value, if provided the cell state is clipped
by this value prior to the cell output activation.</li>
<li><strong>keep_prob</strong>: unit Tensor or float between 0 and 1 representing the
recurrent dropout probability value. If float and 1.0, no dropout will
be applied.</li>
<li><strong>dropout_seed</strong>: (optional) integer, the randomness seed.</li>
<li><strong>trainable</strong>: If <code>True</code> also add variables to the graph collection
<code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li>
<li><strong>outputs_collections</strong>: The collections to which the outputs are added.</li>
</ul>
<hr />
<h1 id="basic-attention-cell">Basic attention cell</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/rnn_cell.py#L161 target="_blank"><b>tefla.core.rnn_cell.AttentionCell</b></a></span>  (cell,  attn_length,  reuse,  trainable=True,  attn_size=None,  attn_vec_size=None,  input_size=None,  layer_norm=None,  layer_norm_args=None,  outputs_collections=None)</span></p>
<p>Implementation based on https://arxiv.org/abs/1409.0473.
Create a cell with attention.</p>
<h3>Args</h3>

<ul>
<li><strong>cell</strong>: an RNNCell, an attention is added to it.
e.g.: a LSTMCell</li>
<li><strong>attn_length</strong>: integer, the size of an attention window.</li>
<li><strong>reuse</strong>: whether or not the layer and its variables should be reused. To be
able to reuse the layer scope must be given.</li>
<li><strong>attn_size</strong>: integer, the size of an attention vector. Equal to
cell.output_size by default.</li>
<li><strong>attn_vec_size</strong>: integer, the number of convolutional features calculated
on attention state and a size of the hidden layer built from
base cell state. Equal attn_size to by default.</li>
<li><strong>input_size</strong>: integer, the size of a hidden linear layer,</li>
<li><strong>layer_norm</strong>: If <code>True</code>, layer normalization will be applied.</li>
<li><strong>layer_norm_args</strong>: optional dict, layer_norm arguments
built from inputs and attention. Derived from the input tensor by default.</li>
<li><strong>trainable</strong>: If <code>True</code> also add variables to the graph collection
<code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li>
<li><strong>outputs_collections</strong>: The collections to which the outputs are added.</li>
</ul>
<hr />
<h1 id="gated-recurrent-unit-cell-cf-httparxivorgabs14061078">Gated Recurrent Unit cell (cf. http://arxiv.org/abs/1406.1078)</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/rnn_cell.py#L247 target="_blank"><b>tefla.core.rnn_cell.GRUCell</b></a></span>  (num_units,  reuse,  trainable=True,  input_size=None,  activation=<function  tanh  at  0x7f0c75162de8>,  inner_activation=<function  sigmoid  at  0x7f0c75162d70>,  b_init=1.0,  layer_norm=None,  layer_norm_args=None,  outputs_collections=None)</span></p>
<h3>Args</h3>

<ul>
<li><strong>num_units</strong>: int, The number of units in the LSTM cell.</li>
<li><strong>reuse</strong>: whether or not the layer and its variables should be reused. To be
able to reuse the layer scope must be given.</li>
<li><strong>input_size</strong>: Deprecated and unused.</li>
<li><strong>activation</strong>: Activation function of the states.</li>
<li><strong>inner_activation</strong>: Activation function of the inner states.</li>
<li><strong>layer_norm</strong>: If <code>True</code>, layer normalization will be applied.</li>
<li><strong>layer_norm_args</strong>: optional dict, layer_norm arguments</li>
<li><strong>trainable</strong>: If <code>True</code> also add variables to the graph collection
<code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li>
<li><strong>outputs_collections</strong>: The collections to which the outputs are added.</li>
</ul>
<hr />
<h1 id="rnn-cell-composed-sequentially-of-multiple-simple-cells">RNN cell composed sequentially of multiple simple cells</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/rnn_cell.py#L309 target="_blank"><b>tefla.core.rnn_cell.MultiRNNCell</b></a></span>  (cells,  state_is_tuple=True)</span></p>
<p>Create a RNN cell composed sequentially of a number of RNNCells.
<h3>Args</h3></p>
<ul>
<li><strong>cells</strong>: list of RNNCells that will be composed in this order.</li>
</ul>
<hr />
<h1 id="operator-adding-dropout-to-inputs-and-outputs-of-the-given-cell">Operator adding dropout to inputs and outputs of the given cell</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/rnn_cell.py#L351 target="_blank"><b>tefla.core.rnn_cell.DropoutWrapper</b></a></span>  (cell,  is_training,  input_keep_prob=1.0,  output_keep_prob=1.0,  seed=None)</span></p>
<p>Create a cell with added input and/or output dropout.
Dropout is never used on the state.</p>
<h3>Args</h3>

<ul>
<li><strong>cell</strong>: an RNNCell, a projection to output_size is added to it.</li>
<li><strong>is_training</strong>: a bool, training if true else validation/testing</li>
<li><strong>input_keep_prob</strong>: unit Tensor or float between 0 and 1, input keep
probability; if it is float and 1, no input dropout will be added.</li>
<li><strong>output_keep_prob</strong>: unit Tensor or float between 0 and 1, output keep
probability; if it is float and 1, no output dropout will be added.</li>
<li><strong>seed</strong>: (optional) integer, the randomness seed.</li>
</ul>
<hr />
<h1 id="adds-a-fully-connected-layer">Adds a fully connected layer</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/rnn_cell.py#L401 target="_blank"><b>tefla.core.rnn_cell._linear</b></a></span>  (x,  n_output,  reuse,  trainable=True,  w_init=<function  _initializer  at  0x7f0c502712a8>,  b_init=0.0,  w_regularizer=<function  l2_loss  at  0x7f0c74fd51b8>,  name='fc',  layer_norm=None,  layer_norm_args=None,  activation=None,  outputs_collections=None,  use_bias=True)</span></p>
<p><code>fully_connected</code> creates a variable called <code>weights</code>, representing a fully
connected weight matrix, which is multiplied by the <code>x</code> to produce a
<code>Tensor</code> of hidden units. If a <code>layer_norm</code> is provided (such as
<code>layer_norm</code>), it is then applied. Otherwise, if <code>layer_norm</code> is
None and a <code>b_init</code> and <code>use_bias</code> is provided then a <code>biases</code> variable would be
created and added the hidden units. Finally, if <code>activation</code> is not <code>None</code>,
it is applied to the hidden units as well.
Note: that if <code>x</code> have a rank greater than 2, then <code>x</code> is flattened
prior to the initial matrix multiply by <code>weights</code>.</p>
<h3>Args</h3>

<ul>
<li><strong>x</strong>: A <code>Tensor</code> of with at least rank 2 and value for the last dimension,
i.e. <code>[batch_size, depth]</code>, <code>[None, None, None, channels]</code>.</li>
<li><strong>n_output</strong>: Integer or long, the number of output units in the layer.</li>
<li><strong>reuse</strong>: whether or not the layer and its variables should be reused. To be
able to reuse the layer scope must be given.</li>
<li><strong>activation</strong>: activation function, set to None to skip it and maintain
a linear activation.</li>
<li><strong>layer_norm</strong>: normalization function to use. If
 -<code>batch_norm</code> is <code>True</code> then google original implementation is used and
if another function is provided then it is applied.
default set to None for no normalizer function</li>
<li><strong>layer_norm_args</strong>: normalization function parameters.</li>
<li><strong>w_init</strong>: An initializer for the weights.</li>
<li><strong>w_regularizer</strong>: Optional regularizer for the weights.</li>
<li><strong>b_init</strong>: An initializer for the biases. If None skip biases.</li>
<li><strong>outputs_collections</strong>: The collections to which the outputs are added.</li>
<li><strong>trainable</strong>: If <code>True</code> also add variables to the graph collection
<code>GraphKeys.TRAINABLE_VARIABLES</code> (see tf.Variable).</li>
<li><strong>name</strong>: Optional name or scope for variable_scope/name_scope.</li>
<li><strong>use_bias</strong>: Whether to add bias or not</li>
</ul>
<h3>Returns</h3>

<p>The 2-D <code>Tensor</code> variable representing the result of the series of operations.
e.g: 2-D <code>Tensor</code> [batch, n_output].</p>
<hr />
<h1 id="adds-a-layer-normalization-layer-from-httpsarxivorgabs160706450">Adds a Layer Normalization layer from https://arxiv.org/abs/1607.06450</h1>
<p><span class="extra_h1"><span style="color:black;"><a href=https://github.com/n3011/tefla/blob/master/tefla/core/rnn_cell.py#L497 target="_blank"><b>tefla.core.rnn_cell.layer_norm</b></a></span>  (x,  reuse,  center=True,  scale=True,  trainable=True,  epsilon=1e-12,  name='bn',  outputs_collections=None)</span>
"Layer Normalization" Jimmy Lei Ba, Jamie Ryan Kiros, Geoffrey E. Hinton
Can be used as a normalizer function for conv2d and fully_connected.</p>
<h3>Args</h3>

<ul>
<li><strong>x</strong>: a tensor with 2 or more dimensions, where the first dimension has
<code>batch_size</code>. The normalization is over all but the last dimension if
<code>data_format</code> is <code>NHWC</code> and the second dimension if <code>data_format</code> is
<code>NCHW</code>.</li>
<li><strong>center</strong>: If True, subtract <code>beta</code>. If False, <code>beta</code> is ignored.</li>
<li><strong>scale</strong>: If True, multiply by <code>gamma</code>. If False, <code>gamma</code> is
not used. When the next layer is linear (also e.g. <code>nn.relu</code>), this can be
disabled since the scaling can be done by the next layer.</li>
<li><strong>epsilon</strong>: small float added to variance to avoid dividing by zero.</li>
<li><strong>reuse</strong>: whether or not the layer and its variables should be reused. To be
able to reuse the layer scope must be given.</li>
<li><strong>outputs_collections</strong>: collections to add the outputs.</li>
<li><strong>trainable</strong>: If <code>True</code> also add variables to the graph collection
<code>GraphKeys.TRAINABLE_VARIABLES</code> (see <code>tf.Variable</code>).</li>
<li><strong>name</strong>: Optional scope/name for <code>variable_scope</code>.</li>
</ul>
<h3>Returns</h3>

<p>A <code>Tensor</code> representing the output of the operation.</p>
<hr />
              
            </div>
          </div>
          <footer>
  
    <div class="rst-footer-buttons" role="navigation" aria-label="footer navigation">
      
        <a href="../special_layers/" class="btn btn-neutral float-right" title="Special Layers">Next <span class="icon icon-circle-arrow-right"></span></a>
      
      
        <a href="../layers/" class="btn btn-neutral" title="Layers"><span class="icon icon-circle-arrow-left"></span> Previous</a>
      
    </div>
  

  <hr/>

  <div role="contentinfo">
    <!-- Copyright etc -->
    
  </div>

  Built with <a href="http://www.mkdocs.org">MkDocs</a> using a <a href="https://github.com/snide/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>.
</footer>
	  
        </div>
      </div>

    </section>
    
  </div>

  <div class="rst-versions" role="note" style="cursor: pointer">
    <span class="rst-current-version" data-toggle="rst-current-version">
      
          <a href="https://github.com/n3011/tefla" class="icon icon-github" style="float: left; color: #fcfcfc"> GitHub</a>
      
      
        <span><a href="../layers/" style="color: #fcfcfc;">&laquo; Previous</a></span>
      
      
        <span style="margin-left: 15px"><a href="../special_layers/" style="color: #fcfcfc">Next &raquo;</a></span>
      
    </span>
</div>
    <script src="../../js/theme.js"></script>

</body>
</html>
